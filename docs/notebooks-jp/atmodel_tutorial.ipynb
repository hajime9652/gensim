{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The author-topic model: LDA with metadata\n",
    "このチュートリアルでは、Gensimで著者 - トピックモデルを使用する方法を学習します。私たちは論文の著者についての洞察を得るために、科学論文からなるコーパスに適用します。\n",
    "\n",
    "著者 - トピックモデルは、潜在的ディリクレ割り当て（LDA）の拡張であり、コーパス内の著者のトピック表現を学習することを可能にします。このモデルは、Web上の投稿のタグなど、ドキュメント上の任意の種類のラベルに適用できます。このモデルは、データ検索の新しい方法、機械学習パイプラインの機能、著者（またはタグ）の予測、または既存のメタデータを使用してトピックモデルを簡単に活用するために使用できます。\n",
    "\n",
    "著者 - トピックモデルの理論的側面については、例えば[Rosen-Zvi and co-authors 2004](https://mimno.infosci.cornell.edu/info6150/readings/398.pdf)を参照してください。 Gensim実装で使用されているアルゴリズムに関するレポートは、すぐに利用可能になります。\n",
    "\n",
    "本チュートリアルでは、もちろん、トピックモデリング、LDA、Gensimに精通していることを前提とします。 LDAまたはそのGensim実装に慣れていない場合は、そこから開始することをお勧めします。これらのリソースのいくつかを考えてみましょう。\n",
    "* LDAモデルへの穏やかな紹介：http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/\n",
    "* GensimのLDA APIドキュメント：https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "Gensimのトピックモデリング：http://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html\n",
    "* [Pre-processing and training LDA](lda_training_tips.ipynb)\n",
    "\n",
    "\n",
    "> **注：**\n",
    ">\n",
    ">このチュートリアルを自分で実行するには、Jupyter、Gensim、SpaCy、Scikit-Learn、Bokeh、Pandasをインストールします。ピップを使用して：\n",
    ">\n",
    "> `pip install jupyter gensim spacy sklearn bokeh pandas`\n",
    ">\n",
    "> `python -m spacy.en.download`を使ってSpaCyのデータをダウンロードする必要があることに注意してください。\n",
    ">\n",
    ">ノートブックをhttps://github.com/RaRe-Technologies/gensim/tree/develop/docs/notebooks/atmodel_tutorial.ipynb からダウンロードしてください。\n",
    "\n",
    "このチュートリアルでは、モデルのデータを準備する方法、モデルを訓練する方法、結果の表現をさまざまな方法で探索する方法について学習します。 Geoffrey HintonやYann LeCunのような有名な作家の話題を調べ、作者を次元削減によってプロットして類似性の比較を行うことで比較します。\n",
    "\n",
    "In this tutorial, you will learn how to use the author-topic model in Gensim. We will apply it to a corpus consisting of scientific papers, to get insight about the authors of the papers.\n",
    "\n",
    "The author-topic model is an extension of Latent Dirichlet Allocation (LDA), that allows us to learn topic representations of authors in a corpus. The model can be applied to any kinds of labels on documents, such as tags on posts on the web. The model can be used as a novel way of data exploration, as features in machine learning pipelines, for author (or tag) prediction, or to simply leverage your topic model with existing metadata.\n",
    "\n",
    "To learn about the theoretical side of the author-topic model, see [Rosen-Zvi and co-authors 2004](https://mimno.infosci.cornell.edu/info6150/readings/398.pdf), for example. A report on the algorithm used in the Gensim implementation will be available soon.\n",
    "\n",
    "Naturally, familiarity with topic modelling, LDA and Gensim is assumed in this tutorial. If you are not familiar with either LDA, or its Gensim implementation, I would recommend starting there. Consider some of these resources:\n",
    "* Gentle introduction to the LDA model: http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/\n",
    "* Gensim's LDA API documentation: https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "* Topic modelling in Gensim: http://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html\n",
    "* [Pre-processing and training LDA](lda_training_tips.ipynb)\n",
    "\n",
    "\n",
    "> **NOTE:**\n",
    ">\n",
    "> To run this tutorial on your own, install Jupyter, Gensim, SpaCy, Scikit-Learn, Bokeh and Pandas, e.g. using pip:\n",
    ">\n",
    "> `pip install jupyter gensim spacy sklearn bokeh pandas`\n",
    ">\n",
    "> Note that you need to download some data for SpaCy using `python -m spacy.en.download`.\n",
    ">\n",
    "> Download the notebook at https://github.com/RaRe-Technologies/gensim/tree/develop/docs/notebooks/atmodel_tutorial.ipynb.\n",
    "\n",
    "In this tutorial, we will learn how to prepare data for the model, how to train it, and how to explore the resulting representation in different ways. We will inspect the topic representation of some well known authors like Geoffrey Hinton and Yann LeCun, and compare authors by plotting them in reduced dimensionality and performing similarity queries.\n",
    "\n",
    "## Analyzing scientific papers\n",
    "私たちが使用するデータは、神経情報処理システム会議（NIPS）の機械学習に関する科学論文から構成されます。前述の[Pre-processing and training LDA](lda_training_tips.ipynb)チュートリアルで使用されているのと同じデータセットです。\n",
    "\n",
    "モデルの定性分析を行い、時にはデータの主題を理解する必要があります。このチュートリアルを自分で実行する場合は、よく知っている内容のデータセットに適用することを検討してください。たとえば、[StackExchange datadump datasets](https://archive.org/details/stackexchange)のいずれかを試してください。\n",
    "\n",
    "Sam Roweisのウェブサイト（http://www.cs.nyu.edu/~roweis/data.html） からデータをダウンロードできます。または、以下のセルを実行するだけで、ダウンロードされ、 `tmpに展開されます。\n",
    "\n",
    "The data we will be using consists of scientific papers about machine learning, from the Neural Information Processing Systems conference (NIPS). It is the same dataset used in the [Pre-processing and training LDA](lda_training_tips.ipynb) tutorial, mentioned earlier.\n",
    "\n",
    "We will be performing qualitative analysis of the model, and at times this will require an understanding of the subject matter of the data. If you try running this tutorial on your own, consider applying it on a dataset with subject matter that you are familiar with. For example, try one of the [StackExchange datadump datasets](https://archive.org/details/stackexchange).\n",
    "\n",
    "You can download the data from Sam Roweis' website (http://www.cs.nyu.edu/~roweis/data.html). Or just run the cell below, and it will be downloaded and extracted into your `tmp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -o - 'http://www.cs.nyu.edu/~roweis/data/nips12raw_str602.tgz' > /tmp/nips12raw_str602.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "filename = './tmp/nips12raw_str602.tgz'\n",
    "tar = tarfile.open(filename, 'r:gz')\n",
    "for item in tar:\n",
    "    tar.extract(item, path='/tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のセクションでは、実装の機能のいくつかを使用して、データをロードし、事前処理し、モデルをトレーニングし、結果を探索します。 このプロセスに精通していれば、今すぐに読み込みと前処理をスキップしても構いません。\n",
    "\n",
    "In the following sections we will load the data, pre-process it, train the model, and explore the results using some of the implementation's functionality. Feel free to skip the loading and pre-processing for now, if you are familiar with the process.\n",
    "\n",
    "### Loading the data\n",
    "下のセルでは、データセット内のフォルダとファイルをクロールし、ファイルをメモリに読み込みます。\n",
    "\n",
    "In the cell below, we crawl the folders and files in the dataset, and read the files into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, io\n",
    "from smart_open import smart_open\n",
    "\n",
    "# Folder containing all NIPS papers.\n",
    "data_dir = './tmp/nipstxt/'  # Set this path to the data on your machine.\n",
    "\n",
    "# Folders containin individual NIPS papers.\n",
    "yrs = ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "dirs = ['nips' + yr for yr in yrs]\n",
    "\n",
    "# Get all document texts and their corresponding IDs.\n",
    "docs = []\n",
    "doc_ids = []\n",
    "for yr_dir in dirs:\n",
    "    files = os.listdir(data_dir + yr_dir)  # List of filenames.\n",
    "    for filen in files:\n",
    "        # Get document ID.\n",
    "        (idx1, idx2) = re.search('[0-9]+', filen).span()  # Matches the indexes of the start end end of the ID.\n",
    "        doc_ids.append(yr_dir[4:] + '_' + str(int(filen[idx1:idx2])))\n",
    "        \n",
    "        # Read document text.\n",
    "        # Note: ignoring characters that cause encoding errors.\n",
    "        with smart_open(data_dir + yr_dir + '/' + filen, 'rb', encoding='utf-8', errors='ignore') as fid:\n",
    "            txt = fid.read()\n",
    "            \n",
    "        # Replace any whitespace (newline, tabs, etc.) by a single space.\n",
    "        txt = re.sub('\\s', ' ', txt)\n",
    "        \n",
    "        docs.append(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "著者名からドキュメントIDへのマッピングを構築します。\n",
    "\n",
    "Construct a mapping from author names to document IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smart_open import smart_open\n",
    "filenames = [data_dir + 'idx/a' + yr + '.txt' for yr in yrs]  # Using the years defined in previous cell.\n",
    "\n",
    "# Get all author names and their corresponding document IDs.\n",
    "author2doc = dict()\n",
    "i = 0\n",
    "for yr in yrs:\n",
    "    # The files \"a00.txt\" and so on contain the author-document mappings.\n",
    "    filename = data_dir + 'idx/a' + yr + '.txt'\n",
    "    for line in smart_open(filename, 'rb', errors='ignore', encoding='utf-8'):\n",
    "        # Each line corresponds to one author.\n",
    "        contents = re.split(',', line)\n",
    "        author_name = (contents[1] + contents[0]).strip()\n",
    "        # Remove any whitespace to reduce redundant author names.\n",
    "        author_name = re.sub('\\s', '', author_name)\n",
    "        # Get document IDs for author.\n",
    "        ids = [c.strip() for c in contents[2:]]\n",
    "        if not author2doc.get(author_name):\n",
    "            # This is a new author.\n",
    "            author2doc[author_name] = []\n",
    "            i += 1\n",
    "        \n",
    "        # Add document IDs to author.\n",
    "        author2doc[author_name].extend([yr + '_' + id for id in ids])\n",
    "\n",
    "# Use an integer ID in author2doc, instead of the IDs provided in the NIPS dataset.\n",
    "# Mapping from ID of document in NIPS datast, to an integer ID.\n",
    "doc_id_dict = dict(zip(doc_ids, range(len(doc_ids))))\n",
    "# Replace NIPS IDs by integer IDs.\n",
    "for a, a_doc_ids in author2doc.items():\n",
    "    for i, doc_id in enumerate(a_doc_ids):\n",
    "        author2doc[a][i] = doc_id_dict[doc_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing text\n",
    "\n",
    "テキストは次の手順で事前処理されます。\n",
    "* テキストをトークン化する。\n",
    "* すべての空白を1つのスペースで置き換えます。\n",
    "* すべての句読点と数字を削除します。\n",
    "* ストップワードを削除します。\n",
    "* 単語を略語にする。\n",
    "* 複数単語の名前付きエンティティを追加します。\n",
    "* 頻繁なバイグラムを追加する。\n",
    "* 頻繁で珍しい言葉を削除する。\n",
    "\n",
    "重たい処理の多くは、素晴らしいパッケージ、Spacyによって行われます。 Spacyは「産業レベルの自然言語処理」として市場に出回っており、高速でマルチプロセッシングが可能で使いやすくなっています。 最初に、それをインポートし、NLPピンプラインを英語で読み込みましょう。\n",
    "\n",
    "The text will be pre-processed using the following steps:\n",
    "* Tokenize text.\n",
    "* Replace all whitespace by single spaces.\n",
    "* Remove all punctuation and numbers.\n",
    "* Remove stopwords.\n",
    "* Lemmatize words.\n",
    "* Add multi-word named entities.\n",
    "* Add frequent bigrams.\n",
    "* Remove frequent and rare words.\n",
    "\n",
    "A lot of the heavy lifting will be done by the great package, Spacy. Spacy markets itself as \"industrial-strength natural language processing\", is fast, enables multiprocessing, and is easy to use. First, let's import it and load the NLP pipline in english."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のコードでは、Spacyはアルファベット以外の文字の削除、ストップワードの削除、字形化および名前付きエンティティ認識をトークン化します。\n",
    "\n",
    "1つの名前のエンティティが既に存在するため、複数の単語で構成される名前付きエンティティのみを保持することに注意してください。\n",
    "\n",
    "In the code below, Spacy takes care of tokenization, removing non-alphabetic characters, removal of stopwords, lemmatization and named entity recognition.\n",
    "\n",
    "Note that we only keep named entities that consist of more than one word, as single word named entities are already there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "processed_docs = []    \n",
    "for doc in nlp.pipe(docs, n_threads=4, batch_size=100):\n",
    "    # Process document using Spacy NLP pipeline.\n",
    "    \n",
    "    ents = doc.ents  # Named entities.\n",
    "\n",
    "    # Keep only words (no numbers, no punctuation).\n",
    "    # Lemmatize tokens, remove punctuation and remove stopwords.\n",
    "    doc = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "    # Remove common words from a stopword list.\n",
    "    #doc = [token for token in doc if token not in STOPWORDS]\n",
    "\n",
    "    # Add named entities, but only if they are a compound of more than word.\n",
    "    doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "    \n",
    "    processed_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = processed_docs\n",
    "del processed_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下では、Gensimモデルを使用してバイグラムを追加します。 これは、名前付きエンティティ認識と同じ目標を達成すること、つまり特に重要な隣接単語を見つけることに注意してください。\n",
    "\n",
    "Below, we use a Gensim model to add bigrams. Note that this achieves the same goal as named entity recognition, that is, finding adjacent words that have some particular significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで、ボキャブラリーが完成したので、辞書を作成する準備が整いました。 次に、一般的な単語（$50 \\%$以上で発生している）とまれな単語（全体で$ 20 $回以下）が削除されます。\n",
    "\n",
    "Now we are ready to construct a dictionary, as our vocabulary is finalized. We then remove common words (occurring $> 50\\%$ of the time), and rare words (occur $< 20$ times in total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents, and filter out frequent and rare words.\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Remove rare and common tokens.\n",
    "# Filter out words that occur too frequently or too rarely.\n",
    "max_freq = 0.5\n",
    "min_wordcount = 20\n",
    "dictionary.filter_extremes(no_below=min_wordcount, no_above=max_freq)\n",
    "\n",
    "_ = dictionary[0]  # This sort of \"initializes\" dictionary.id2token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoWを計算することによって、著者 - トピックモデルを供給するために、文書のベクトル化された表現を生成する。\n",
    "\n",
    "We produce the vectorized representation of the documents, to supply the author-topic model with, by computing the bag-of-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vectorize data.\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "私たちのデータの次元を調べましょう。\n",
    "\n",
    "Let's inspect the dimensionality of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of authors: %d' % len(author2doc))\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and use model\n",
    "\n",
    "私たちは、前のセクションで準備したデータをもとに著者 - トピックモデルを訓練します。\n",
    "\n",
    "著者 - トピックモデルへのインタフェースは、GensimのLDAのインタフェースに非常に似ています。 著者 - トピックモデルには、コーパス、IDから単語へのマッピング（ `id2word`）、トピックの数（` num_topics`）の他に、IDマッピングを文書化する（ `author2doc`）か、もとに戻す（` doc2author `）。\n",
    "\n",
    "以下では、これもスキップしています。\n",
    "* 最適化問題の収束を向上させるために、データセットに対する「パス」の数を増やしました。\n",
    "* 各文書の「反復回数」を減らしました（上記に関連しています）。\n",
    "* ミニバッチサイズ（ `chunksize`）を指定しました（主にトレーニングをスピードアップするため）。\n",
    "* バウンド評価（ `eval_every`）をオフにしました（計算に時間がかかるため）。\n",
    "* 最適化問題の収束を改善するために、 `alpha`と` eta`priorsの自動学習を有効にしました。\n",
    "* 乱数ジェネレータのランダムな状態（ `random_state`）を設定します（これらの実験を再現可能にするため）。\n",
    "\n",
    "モデルをロードしてトレーニングします。\n",
    "\n",
    "We train the author-topic model on the data prepared in the previous sections. \n",
    "\n",
    "The interface to the author-topic model is very similar to that of LDA in Gensim. In addition to a corpus, ID to word mapping (`id2word`) and number of topics (`num_topics`), the author-topic model requires either an author to document ID mapping (`author2doc`), or the reverse (`doc2author`).\n",
    "\n",
    "Below, we have also (this can be skipped for now):\n",
    "* Increased the number of `passes` over the dataset (to improve the convergence of the optimization problem).\n",
    "* Decreased the number of `iterations` over each document (related to the above).\n",
    "* Specified the mini-batch size (`chunksize`) (primarily to speed up training).\n",
    "* Turned off bound evaluation (`eval_every`) (as it takes a long time to compute).\n",
    "* Turned on automatic learning of the `alpha` and `eta` priors (to improve the convergence of the optimization problem).\n",
    "* Set the random state (`random_state`) of the random number generator (to make these experiments reproducible).\n",
    "\n",
    "We load the model, and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import AuthorTopicModel\n",
    "%time model = AuthorTopicModel(corpus=corpus, num_topics=10, id2word=dictionary.id2token, \\\n",
    "                author2doc=author2doc, chunksize=2000, passes=1, eval_every=0, \\\n",
    "                iterations=1, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルが収束していないと思われる場合は、 `model.update（）`を使ってトレーニングを続けることができます。 追加の文書や著者がいる場合は `model.update（corpus、author2doc）`を呼び出します。\n",
    "\n",
    "モデルを調べる前に、そのモデルを改善しようとしましょう。 これを行うために、私たちは乱数ジェネレータ（ `random_state`）に異なる種を与えることによって、異なるランダム初期化を持ついくつかのモデルを訓練します。 [top_topics](https://radimrehurek.com/gensim/models/ldamodel.html#gensim.models.ldamodel.LdaModel.top_topics)メソッドを使用してトピックの一貫性を評価し、最もトピックのコンシステンシーが高いモデルを選択します\n",
    "\n",
    "If you believe your model hasn't converged, you can continue training using `model.update()`. If you have additional documents and/or authors call `model.update(corpus, author2doc)`.\n",
    "\n",
    "Before we explore the model, let's try to improve upon it. To do this, we will train several models with different random initializations, by giving different seeds for the random number generator (`random_state`). We evaluate the topic coherence of the model using the [top_topics](https://radimrehurek.com/gensim/models/ldamodel.html#gensim.models.ldamodel.LdaModel.top_topics) method, and pick the model with the highest topic coherence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_list = []\n",
    "for i in range(5):\n",
    "    model = AuthorTopicModel(corpus=corpus, num_topics=10, id2word=dictionary.id2token, \\\n",
    "                    author2doc=author2doc, chunksize=2000, passes=100, gamma_threshold=1e-10, \\\n",
    "                    eval_every=0, iterations=1, random_state=i)\n",
    "    top_topics = model.top_topics(corpus)\n",
    "    tc = sum([t[1] for t in top_topics])\n",
    "    model_list.append((model, tc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トピックのコンシステンシーが最も高いモデルを選択します。\n",
    "\n",
    "Choose the model with the highest topic coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tc = max(model_list, key=lambda x: x[1])\n",
    "print('Topic coherence: %.3e' %tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルを保存して、再度訓練する必要がなくなる、再度ロードする方法も示します。\n",
    "\n",
    "We save the model, to avoid having to train it again, and also show how to load it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model.\n",
    "model.save('/tmp/model.atmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model.\n",
    "model = AuthorTopicModel.load('/tmp/model.atmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore author-topic representation\n",
    "\n",
    "モデルを訓練したので、著者とトピックを調べ始めることができます。\n",
    "\n",
    "まず、トピックで最も重要な単語を単に出力してみましょう。 以下では、トピック0を印刷しています。各トピックは単語のセットに関連付けられており、各単語はそのトピックで表現される可能性があります。\n",
    "\n",
    "Now that we have trained a model, we can start exploring the authors and the topics.\n",
    "\n",
    "First, let's simply print the most important words in the topics. Below we have printed topic 0. As we can see, each topic is associated with a set of words, and each word has a probability of being expressed under that topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.show_topic(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下では、それぞれのトピックに直感的に近いものと思われるものに基づいて、各トピックにラベルを付けました。\n",
    "\n",
    "Below, we have given each topic a label based on what each topic seems to be about intuitively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_labels = ['Circuits', 'Neuroscience', 'Numerical optimization', 'Object recognition', \\\n",
    "               'Math/general', 'Robotics', 'Character recognition', \\\n",
    "                'Reinforcement learning', 'Speech recognition', 'Bayesian modelling']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単に `model.show_topics（num_topics = 10）`を呼び出すのではなく、出力を少しフォーマットして、概要を得る方が簡単です。\n",
    "\n",
    "Rather than just calling `model.show_topics(num_topics=10)`, we format the output a bit so it is easier to get an overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in model.show_topics(num_topics=10):\n",
    "    print('Label: ' + topic_labels[topic[0]])\n",
    "    words = ''\n",
    "    for word, prob in model.show_topic(topic[0]):\n",
    "        words += word + ' '\n",
    "    print('Words: ' + words)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これらのトピックは決して完璧ではありません。 *連載トピック*、*侵入単語*、*ランダムトピック*、*不均衡トピック*（ [Mimno and co-authors 2011](https://people.cs.umass.edu/~wallach/publications/mimno11optimizing.pdf)）のような問題を抱えています。 しかし、このチュートリアルの目的のために行います。以下では、 `model [name]`構文を使用して、著者のトピックの分布を取得します。 各トピックには、特定の著者が与えられたときに表現される確率がありますが、特定のしきい値を超えるトピックのみが表示されます。\n",
    "\n",
    "These topics are by no means perfect. They have problems such as *chained topics*, *intruded words*, *random topics*, and *unbalanced topics* (see [Mimno and co-authors 2011](https://people.cs.umass.edu/~wallach/publications/mimno11optimizing.pdf)). They will do for the purposes of this tutorial, however.\n",
    "\n",
    "Below, we use the `model[name]` syntax to retrieve the topic distribution for an author. Each topic has a probability of being expressed given the particular author, but only the ones above a certain threshold are shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['YannLeCun']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "いくつかの著者のトップトピックを出力しましょう。 まず、これをより簡単に行うための機能を作ります。\n",
    "\n",
    "Let's print the top topics of some authors. First, we make a function to help us do this more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def show_author(name):\n",
    "    print('\\n%s' % name)\n",
    "    print('Docs:', model.author2doc[name])\n",
    "    print('Topics:')\n",
    "    pprint([(topic_labels[topic[0]], topic[1]) for topic in model[name]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下では、高プロファイルの研究者をいくつか出力し、それらを検査します。 これらのうちの3つ、Yann LeCun、Geoffrey E. Hinton、Christof Kochがあります。\n",
    "\n",
    "しかしTerrence J. Sejnowskiの結果は驚くべきことである。 彼は神経科学者ですから、彼は \"神経科学\"のラベルを得ると期待しています。 これは、Sejnowskiが視覚認知の神経科学の側面で働いていること、またはおそらくトピックを誤ってラベル付けしていること、あるいは単にこのトピックがあまり有益ではないことを示している可能性があります。\n",
    "\n",
    "Below, we print some high profile researchers and inspect them. Three of these, Yann LeCun, Geoffrey E. Hinton and Christof Koch, are spot on. \n",
    "\n",
    "Terrence J. Sejnowski's results are surprising, however. He is a neuroscientist, so we would expect him to get the \"neuroscience\" label. This may indicate that Sejnowski works with the neuroscience aspects of visual perception, or perhaps that we have labeled the topic incorrectly, or perhaps that this topic simply is not very informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_author('YannLeCun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_author('GeoffreyE.Hinton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_author('TerrenceJ.Sejnowski')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_author('ChristofKoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple model evaluation methods\n",
    "モデルの予測パフォーマンスの尺度である単語単位の境界を計算することができます（再構成エラーとも言えます）。\n",
    "\n",
    "そのためには、自動的に構築できる `doc2author`辞書が必要です。\n",
    "\n",
    "\n",
    "We can compute the per-word bound, which is a measure of the model's predictive performance (you could also say that it is the reconstruction error).\n",
    "\n",
    "To do that, we need the `doc2author` dictionary, which we can build automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import atmodel\n",
    "doc2author = atmodel.construct_doc2author(model.corpus, model.author2doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では、単語単位の境界を評価しましょう。\n",
    "\n",
    "Now let's evaluate the per-word bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the per-word bound.\n",
    "# Number of words in corpus.\n",
    "corpus_words = sum(cnt for document in model.corpus for _, cnt in document)\n",
    "\n",
    "# Compute bound and divide by number of words.\n",
    "perwordbound = model.bound(model.corpus, author2doc=model.author2doc, \\\n",
    "                           doc2author=model.doc2author) / corpus_words\n",
    "print(perwordbound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDAクラスのように、トピックのcoherenceを計算することによって、トピックの品質を評価することができます。 これをたとえばに使用します。 どのトピックが低品質であるか、またはモデル選択の基準として見つけます。\n",
    "\n",
    "We can evaluate the quality of the topics by computing the topic coherence, as in the LDA class. Use this to e.g. find out which of the topics are poor quality, or as a metric for model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time top_topics = model.top_topics(model.corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the authors\n",
    "今、私たちは、太平洋列島のような種類のプロットを次のように作成します。 このプロットの目的は、著者トピック表現を直観的に探求する方法を提供することです。\n",
    "\n",
    "私たちは、（`model.state.gamma`に格納されている）すべての著者 - トピック分布をとり、それらを2D空間に埋め込みます。 これを行うために、t-SNEを使用してこのデータの次元性を減らします。\n",
    "\n",
    "t-SNEは、ポイント間の距離を維持しながら、データセットの次元数を減らそうとする方法です。 つまり、2人の著者が下のプロットで近くにいると、そのトピックの分布は似ています。\n",
    "\n",
    "以下のセルでは、著者 - トピック表現をt-SNE空間に変換します。 少数の文書ですべての著者を表示したくない場合は、 `smallest_author`値を増やすことができます。\n",
    "\n",
    "\n",
    "Now we're going to produce the kind of pacific archipelago looking plot below. The goal of this plot is to give you a way to explore the author-topic representation in an intuitive manner.\n",
    "\n",
    "We take all the author-topic distributions (stored in `model.state.gamma`) and embed them in a 2D space. To do this, we reduce the dimensionality of this data using t-SNE. \n",
    "\n",
    "t-SNE is a method that attempts to reduce the dimensionality of a dataset, while maintaining the distances between the points. That means that if two authors are close together in the plot below, then their topic distributions are similar.\n",
    "\n",
    "In the cell below, we transform the author-topic representation into the t-SNE space. You can increase the `smallest_author` value if you do not want to view all the authors with few documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "smallest_author = 0  # Ignore authors with documents less than this.\n",
    "authors = [model.author2id[a] for a in model.author2id.keys() if len(model.author2doc[a]) >= smallest_author]\n",
    "_ = tsne.fit_transform(model.state.gamma[authors, :])  # Result stored in tsne.embedding_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我々は今、プロットをする準備が整いました。\n",
    "\n",
    "このノートブックを自分で実行すると、別のグラフが表示されます。 モデルのランダムな初期化は異なるため、結果はある程度異なります。 データの表現がまったく異なる場合や、同じ解釈が若干異なる場合があります。\n",
    "\n",
    "プロットが見えない場合は、Jupiterノートでこのチュートリアルを表示している可能性があります。 http://nbviewer.jupyter.org/github/rare-technologies/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb　の代わりにnbviewerで表示してください。\n",
    "\n",
    "We are now ready to make the plot.\n",
    "\n",
    "Note that if you run this notebook yourself, you will see a different graph. The random initialization of the model will be different, and the result will thus be different to some degree. You may find an entirely different representation of the data, or it may show the same interpretation slightly differently.\n",
    "\n",
    "If you can't see the plot, you are probably viewing this tutorial in a Jupyter Notebook. View it in an nbviewer instead at http://nbviewer.jupyter.org/github/rare-technologies/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tell Bokeh to display plots inside the notebook.\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import HoverTool\n",
    "from bokeh.plotting import figure, show, ColumnDataSource\n",
    "\n",
    "x = tsne.embedding_[:, 0]\n",
    "y = tsne.embedding_[:, 1]\n",
    "author_names = [model.id2author[a] for a in authors]\n",
    "\n",
    "# Radius of each point corresponds to the number of documents attributed to that author.\n",
    "scale = 0.1\n",
    "author_sizes = [len(model.author2doc[a]) for a in author_names]\n",
    "radii = [size * scale for size in author_sizes]\n",
    "\n",
    "source = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            author_names=author_names,\n",
    "            author_sizes=author_sizes,\n",
    "            radii=radii,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Add author names and sizes to mouse-over info.\n",
    "hover = HoverTool(\n",
    "        tooltips=[\n",
    "        (\"author\", \"@author_names\"),\n",
    "        (\"size\", \"@author_sizes\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "p = figure(tools=[hover, 'crosshair,pan,wheel_zoom,box_zoom,reset,save,lasso_select'])\n",
    "p.scatter('x', 'y', radius='radii', source=source, fill_alpha=0.6, line_color=None)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上のプロットの円は個々の著者であり、そのサイズは対応する著者に帰属する文書の数を表します。マウスをサークル上に置くと、著者の名前とサイズがわかります。著者の大きなクラスターは関心のある重複を反映する傾向があります。\n",
    "\n",
    "このモデルは、重複する著者を密接させる傾向があることがわかります。たとえば、Terrence J. SejnowkiとT. J. Sejnowskiは同じ人物で、そのベクトルは同じ場所にあります（プロットの$（-10、-10）$を参照）。\n",
    "\n",
    "約$（〜15、-10）$では、Christof KochやJames M. Bowerのような神経科学者の集団があります。\n",
    "\n",
    "先に述べたように、「物体認識」のトピックはSejnowskiに割り当てられました。 Sejnoskiの近隣のピーター・ダーヤン（Peter Dayan）のような他の作家の話題を取り上げると、同じ話題があります。さらに、このクラスターは上記の「神経科学」クラスターに近く、このトピックは脳における視覚知覚に関するものであることがさらに分かります。\n",
    "\n",
    "他のクラスターには、約$（-5,8）$の強化学習クラスターと約$（8、-12）$のベイジアンモデルクラスターがあります。\n",
    "\n",
    "The circles in the plot above are individual authors, and their sizes represent the number of documents attributed to the corresponding author. Hovering your mouse over the circles will tell you the name of the authors and their sizes. Large clusters of authors tend to reflect some overlap in interest. \n",
    "\n",
    "We see that the model tends to put duplicate authors close together. For example, Terrence J. Sejnowki and T. J. Sejnowski are the same person, and their vectors end up in the same place (see about $(-10, -10)$ in the plot).\n",
    "\n",
    "At about $(-15, -10)$ we have a cluster of neuroscientists like Christof Koch and James M. Bower. \n",
    "\n",
    "As discussed earlier, the \"object recognition\" topic was assigned to Sejnowski. If we get the topics of the other authors in Sejnoski's neighborhood, like Peter Dayan, we also get this same topic. Furthermore, we see that this cluster is close to the \"neuroscience\" cluster discussed above, which is further indication that this topic is about visual perception in the brain.\n",
    "\n",
    "Other clusters include a reinforcement learning cluster at about $(-5, 8)$, and a Bayesian modelling cluster at about $(8, -12)$.\n",
    "\n",
    "#### Similarity queries\n",
    "\n",
    "このセクションでは、著者の名前をとり、最も似ている著者を生み出すシステムを構築しようとしています。 この機能性は、情報検索（すなわち、何らかの検索エンジン）の構成要素として、または作成者予測システム、すなわち、ラベル付けされていない文書を取り上げ、それを書いた著者を予測するシステムで使用することができる。\n",
    "\n",
    "著者 - トピック空間で最も近いベクトルを検索するだけでよい。 この意味で、アプローチは上記のt-SNEプロットと似ています。\n",
    "\n",
    "以下に、Gensimの組み込みの類似性フレームワークを使用した類似性クエリを示します。\n",
    "\n",
    "In this section, we are going to set up a system that takes the name of an author and yields the authors that are most similar. This functionality can be used as a component in an information retrieval (i.e. a search engine of some kind), or in an author prediction system, i.e. a system that takes an unlabelled document and predicts the author(s) that wrote it.\n",
    "\n",
    "We simply need to search for the closest vector in the author-topic space. In this sense, the approach is similar to the t-SNE plot above.\n",
    "\n",
    "Below we illustrate a similarity query using a built-in similarity framework in Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.similarities import MatrixSimilarity\n",
    "\n",
    "# Generate a similarity object for the transformed corpus.\n",
    "index = MatrixSimilarity(model[list(model.id2author.values())])\n",
    "\n",
    "# Get similarities to some author.\n",
    "author_name = 'YannLeCun'\n",
    "sims = index[model[author_name]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "しかし、このフレームワークはコサイン距離を使用しますが、Hellinger距離を使用します。 Hellinger距離は、2つの確率分布間の距離（すなわち不一致）を測定する自然な方法である。 その離散バージョンは、\n",
    "$$\n",
    "H(p, q) = \\frac{1}{\\sqrt{2}} \\sqrt{\\sum_{i=1}^K (\\sqrt{p_i} - \\sqrt{q_i})^2},\n",
    "$$\n",
    "ここで、$ p $と$ q $は、2人の異なる著者の両方のトピック配布です。 類似度は次のように定義します。\n",
    "$$\n",
    "S(p, q) = \\frac{1}{1 + H(p, q)}.\n",
    "$$\n",
    "下のセルでは、Hellinger距離に基づいて類似クエリを実行するために必要なものすべてを準備します。\n",
    "\n",
    "However, this framework uses the cosine distance, but we want to use the Hellinger distance. The Hellinger distance is a natural way of measuring the distance (i.e. dis-similarity) between two probability distributions. Its discrete version is defined as\n",
    "$$\n",
    "H(p, q) = \\frac{1}{\\sqrt{2}} \\sqrt{\\sum_{i=1}^K (\\sqrt{p_i} - \\sqrt{q_i})^2},\n",
    "$$\n",
    "\n",
    "where $p$ and $q$ are both topic distributions for two different authors. We define the similarity as\n",
    "$$\n",
    "S(p, q) = \\frac{1}{1 + H(p, q)}.\n",
    "$$\n",
    "\n",
    "In the cell below, we prepare everything we need to perform similarity queries based on the Hellinger distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a function that returns similarities based on the Hellinger distance.\n",
    "\n",
    "from gensim import matutils\n",
    "import pandas as pd\n",
    "\n",
    "# Make a list of all the author-topic distributions.\n",
    "author_vecs = [model.get_author_topics(author) for author in model.id2author.values()]\n",
    "\n",
    "def similarity(vec1, vec2):\n",
    "    '''Get similarity between two vectors'''\n",
    "    dist = matutils.hellinger(matutils.sparse2full(vec1, model.num_topics), \\\n",
    "                              matutils.sparse2full(vec2, model.num_topics))\n",
    "    sim = 1.0 / (1.0 + dist)\n",
    "    return sim\n",
    "\n",
    "def get_sims(vec):\n",
    "    '''Get similarity of vector to all authors.'''\n",
    "    sims = [similarity(vec, vec2) for vec2 in author_vecs]\n",
    "    return sims\n",
    "\n",
    "def get_table(name, top_n=10, smallest_author=1):\n",
    "    '''\n",
    "    Get table with similarities, author names, and author sizes.\n",
    "    Return `top_n` authors as a dataframe.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Get similarities.\n",
    "    sims = get_sims(model.get_author_topics(name))\n",
    "\n",
    "    # Arrange author names, similarities, and author sizes in a list of tuples.\n",
    "    table = []\n",
    "    for elem in enumerate(sims):\n",
    "        author_name = model.id2author[elem[0]]\n",
    "        sim = elem[1]\n",
    "        author_size = len(model.author2doc[author_name])\n",
    "        if author_size >= smallest_author:\n",
    "            table.append((author_name, sim, author_size))\n",
    "            \n",
    "    # Make dataframe and retrieve top authors.\n",
    "    df = pd.DataFrame(table, columns=['Author', 'Score', 'Size'])\n",
    "    df = df.sort_values('Score', ascending=False)[:top_n]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今、特定の作者に最も類似した著者を見つけることができます。 我々はパンダのライブラリを使用して、見栄えの良い表で結果を出力します。\n",
    "\n",
    "Now we can find the most similar authors to some particular author. We use the Pandas library to print the results in a nice looking tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_table('YannLeCun')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前と同じように、著者の最小サイズを指定することができます。\n",
    "\n",
    "As before, we can specify the minimum author size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_table('JamesM.Bower', smallest_author=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Serialized corpora\n",
    "\n",
    "`AuthorTopicModel`クラスは、直列化されたコーパス、すなわちメモリではなくハードドライブに格納されたコーパスを受け入れます。これは通常、コーパスが大きすぎてメモリに収まらない場合に実行されます。ただし、この機能にはいくつかの注意点がありますが、ここで説明します。これらの警告は、この機能を理想よりも低くしているため、将来的には改善される可能性があります。\n",
    "\n",
    "シリアライズされたコーパスを使用するつもりがない場合は、このセクションを読む必要はありません。\n",
    "\n",
    "以下では、説明の後に例と要約を示す。\n",
    "\n",
    "コーパスがシリアライズされている場合、ユーザーは `serialized = True`を指定する必要があります。入力コーパスは、任意のタイプの反復可能または生成元になります。\n",
    "\n",
    "モデルは入力コーパスを受け取り、 `MmCorpus`フォーマットでシリアル化します。これは[Gensimでサポートされています]（https://radimrehurek.com/gensim/corpora/mmcorpus.html）です。\n",
    "\n",
    "モデルでは、すべての入力ドキュメントをシリアル化するパスを指定する必要があります（例： `serialization_path = '/ tmp / model_serializer.mm'`)。重要なデータを誤って上書きしないようにするため、 `serialization_path`にファイルがすでに存在する場合、モデルはエラーを発生させます。この場合、別のパスを選択するか、古いファイルを削除してください。\n",
    "\n",
    "新しいデータを訓練し、 `model.update（corpus、author2doc）`を呼び出す場合は、古いデータと新しいデータをすべて再直列化する必要があります。これはもちろん計算的に要求の厳しいものである可能性があるので、必要な場合にのみ*これを行うことをお勧めします。つまり、新しいドキュメントごとにモデルを更新するのではなく、できるだけ多くの新しいデータが更新されるまで待ってください。\n",
    "\n",
    "The `AuthorTopicModel` class accepts serialized corpora, that is, corpora that are stored on the hard-drive rather than in memory. This is usually done when the corpus is too big to fit in memory. There are, however, some caveats to this functionality, which we will discuss here. As these caveats make this functionality less than ideal, it may be improved in the future.\n",
    "\n",
    "It is not necessary to read this section if you don't intend to use serialized corpora.\n",
    "\n",
    "In the following, an explanation, followed by an example and a summarization will be given.\n",
    "\n",
    "If the corpus is serialized, the user must specify `serialized=True`. Any input corpus can then be any type of iterable or generator.\n",
    "\n",
    "The model will then take the input corpus and serialize it in the `MmCorpus` format, which is [supported in Gensim](https://radimrehurek.com/gensim/corpora/mmcorpus.html).\n",
    "\n",
    "The user must specify the path where the model should serialize all input documents, for example `serialization_path='/tmp/model_serializer.mm'`. To avoid accidentally overwriting some important data, the model will raise an error if there already exists a file at `serialization_path`; in this case, either choose another path, or delete the old file.\n",
    "\n",
    "When you want to train on new data, and call `model.update(corpus, author2doc)`, all the old data and the new data have to be re-serialized. This can of course be quite computationally demanding, so it is recommended that you do this *only* when necessary; that is, wait until you have as much new data as possible to update, rather than updating the model for every new document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time model_ser = AuthorTopicModel(corpus=corpus, num_topics=10, id2word=dictionary.id2token, \\\n",
    "                               author2doc=author2doc, random_state=1, serialized=True, \\\n",
    "                               serialization_path='/tmp/model_serialization.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the file, once you're done using it.\n",
    "import os\n",
    "os.remove('/tmp/model_serialization.mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要約すると、シリアライズされたコーパスを使用する場合：\n",
    "* `serialized = True`を設定します。\n",
    "* `serialization_path`をまだファイルが入っていないパスに設定します。\n",
    "* `model.update（corpus、author2doc）`を呼び出す前に、たくさんのデータがあるまで待ってください。\n",
    "*完了したら、それがもはや必要でないなら `serialization_path`でファイルを削除してください。\n",
    "\n",
    "In summary, when using serialized corpora:\n",
    "* Set `serialized=True`.\n",
    "* Set `serialization_path` to a path that doesn't already contain a file.\n",
    "* Wait until you have lots of data before you call `model.update(corpus, author2doc)`.\n",
    "* When done, delete the file at `serialization_path` if it's not needed anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to try next\n",
    "\n",
    "[StackExchange data dump](https://archive.org/details/stackexchange)のデータセットの1つでモデルを試してください。 記事のタグを著者として扱い、 \"タグ - トピック\"モデルを鍛えることができます。 統計から料理、哲学まで、さまざまなカテゴリがありますので、好きなものを選ぶことができます。 このデータセットのタグを使用する[Kaggle competition](https://www.kaggle.com/c/transfer-learning-on-stack-exchange-tags)を試すことさえできます。\n",
    "\n",
    "Try the model on one of the datasets in the [StackExchange data dump](https://archive.org/details/stackexchange). You can treat the tags on the posts as authors and train a \"tag-topic\" model. There are many different categories, from statistics to cooking to philosophy, so you can pick on that you like. You can even try your hand at a [Kaggle competition](https://www.kaggle.com/c/transfer-learning-on-stack-exchange-tags) that uses tags in this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
